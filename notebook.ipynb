{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pos-tag.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GoazWq8ppxPC"
      },
      "source": [
        "# Chinese POS Tag\n",
        "\n",
        "[Dataset](https://github.com/UniversalDependencies/UD_Chinese-GSDSimp/tree/master)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSJzNGyYp8g9"
      },
      "source": [
        "## Explore dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJ7QEOJ7qBcW"
      },
      "source": [
        "### Download dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ECQM2DFFiJCp",
        "outputId": "e7f09fd2-4202-4d20-cd10-626ef6a65390"
      },
      "source": [
        "!git clone https://github.com/UniversalDependencies/UD_Chinese-GSDSimp.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'UD_Chinese-GSDSimp'...\n",
            "remote: Enumerating objects: 220, done.\u001b[K\n",
            "remote: Counting objects: 100% (193/193), done.\u001b[K\n",
            "remote: Compressing objects: 100% (122/122), done.\u001b[K\n",
            "remote: Total 220 (delta 136), reused 125 (delta 71), pack-reused 27\u001b[K\n",
            "Receiving objects: 100% (220/220), 1.93 MiB | 2.78 MiB/s, done.\n",
            "Resolving deltas: 100% (143/143), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "083slYHUqGVG"
      },
      "source": [
        "### Read dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1FPCbAn1iE-A",
        "outputId": "cdb0b393-d1c0-436b-f218-091499085775"
      },
      "source": [
        "!pip install conllu"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting conllu\n",
            "  Downloading conllu-4.4.1-py2.py3-none-any.whl (15 kB)\n",
            "Installing collected packages: conllu\n",
            "Successfully installed conllu-4.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4yfQYi1viN47",
        "outputId": "2fe1c1c3-f82d-458e-90f0-f15ba664b8d7"
      },
      "source": [
        "from conllu import parse, parse_incr\n",
        "from io import open\n",
        "\n",
        "train_data = open(\"UD_Chinese-GSDSimp/zh_gsdsimp-ud-train.conllu\", \"r\", encoding=\"utf-8\")\n",
        "test_data = open(\"UD_Chinese-GSDSimp/zh_gsdsimp-ud-test.conllu\", \"r\", encoding=\"utf-8\")\n",
        "\n",
        "train_sentences = parse_incr(train_data)\n",
        "test_sentences = parse_incr(test_data)\n",
        "\n",
        "train_sentences = list(train_sentences)\n",
        "test_sentences = list(test_sentences)\n",
        "\n",
        "print(train_sentences[0].serialize())"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# sent_id = train-s1\n",
            "# text = 看似简单，只是二选一做决择，但其实他们代表的是你周遭的亲朋好友，试着给你不同的意见，但追根究底，最后决定的还是自己。\n",
            "1\t看似\t看似\tAUX\tVV\t_\t2\tcop\t_\tSpaceAfter=No\n",
            "2\t简单\t简单\tADJ\tJJ\t_\t5\tadvcl\t_\tSpaceAfter=No\n",
            "3\t，\t，\tPUNCT\t,\t_\t5\tpunct\t_\tSpaceAfter=No\n",
            "4\t只\t只\tADV\tRB\t_\t5\tadvmod\t_\tSpaceAfter=No\n",
            "5\t是\t是\tVERB\tVC\t_\t0\troot\t_\tSpaceAfter=No\n",
            "6\t二\t二\tNUM\tCD\tNumType=Card\t7\tadvmod\t_\tSpaceAfter=No\n",
            "7\t选\t选\tVERB\tVV\t_\t9\tadvcl\t_\tSpaceAfter=No\n",
            "8\t一\t一\tNUM\tCD\tNumType=Card\t7\tobj\t_\tSpaceAfter=No\n",
            "9\t做\t做\tVERB\tVV\t_\t5\txcomp\t_\tSpaceAfter=No\n",
            "10\t决择\t决择\tNOUN\tNN\t_\t9\tobj\t_\tSpaceAfter=No\n",
            "11\t，\t，\tPUNCT\t,\t_\t5\tpunct\t_\tSpaceAfter=No\n",
            "12\t但\t但\tADV\tRB\t_\t22\tmark\t_\tSpaceAfter=No\n",
            "13\t其实\t其实\tADV\tRB\t_\t22\tadvmod\t_\tSpaceAfter=No\n",
            "14\t他们\t他\tPRON\tPRP\tNumber=Plur|Person=3\t15\tnsubj\t_\tSpaceAfter=No\n",
            "15\t代表\t代表\tVERB\tVV\t_\t22\tcsubj\t_\tSpaceAfter=No\n",
            "16\t的\t的\tPART\tDEC\t_\t15\tmark:rel\t_\tSpaceAfter=No\n",
            "17\t是\t是\tAUX\tVC\t_\t22\tcop\t_\tSpaceAfter=No\n",
            "18\t你\t你\tPRON\tPRP\tPerson=2\t19\tnsubj\t_\tSpaceAfter=No\n",
            "19\t周遭\t周遭\tNOUN\tNN\t_\t22\tnmod\t_\tSpaceAfter=No\n",
            "20\t的\t的\tPART\tDEC\tCase=Gen\t19\tcase\t_\tSpaceAfter=No\n",
            "21\t亲朋\t亲朋\tNOUN\tNN\t_\t22\tnmod\t_\tSpaceAfter=No\n",
            "22\t好友\t好友\tNOUN\tNN\t_\t5\tparataxis\t_\tSpaceAfter=No\n",
            "23\t，\t，\tPUNCT\t,\t_\t5\tpunct\t_\tSpaceAfter=No\n",
            "24\t试\t试\tVERB\tVV\t_\t5\tparataxis\t_\tSpaceAfter=No\n",
            "25\t着\t着\tAUX\tAS\tAspect=Prog\t24\taux\t_\tSpaceAfter=No\n",
            "26\t给\t给\tVERB\tVV\t_\t24\txcomp\t_\tSpaceAfter=No\n",
            "27\t你\t你\tPRON\tPRP\tPerson=2\t26\tiobj\t_\tSpaceAfter=No\n",
            "28\t不同\t不同\tADJ\tJJ\t_\t30\tamod\t_\tSpaceAfter=No\n",
            "29\t的\t的\tPART\tDEC\t_\t28\tmark:rel\t_\tSpaceAfter=No\n",
            "30\t意见\t意见\tNOUN\tNN\t_\t26\tobj\t_\tSpaceAfter=No\n",
            "31\t，\t，\tPUNCT\t,\t_\t5\tpunct\t_\tSpaceAfter=No\n",
            "32\t但\t但\tADV\tRB\t_\t39\tmark\t_\tSpaceAfter=No\n",
            "33\t追根究底\t追根究底\tVERB\tVV\t_\t39\tacl\t_\tSpaceAfter=No\n",
            "34\t，\t，\tPUNCT\t,\t_\t39\tpunct\t_\tSpaceAfter=No\n",
            "35\t最后\t最后\tNOUN\tNN\t_\t36\tnmod:tmod\t_\tSpaceAfter=No\n",
            "36\t决定\t决定\tVERB\tVV\t_\t39\tcsubj\t_\tSpaceAfter=No\n",
            "37\t的\t的\tPART\tDEC\t_\t36\tmark:rel\t_\tSpaceAfter=No\n",
            "38\t还是\t还是\tAUX\tVC\t_\t39\tcop\t_\tSpaceAfter=No\n",
            "39\t自己\t自己\tPRON\tPRD\t_\t5\tparataxis\t_\tSpaceAfter=No\n",
            "40\t。\t。\tPUNCT\t.\t_\t5\tpunct\t_\tSpaceAfter=No\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEjgkUtHkIMy"
      },
      "source": [
        "train_tokens = [[token['lemma'] for token in senc] for senc in train_sentences]\n",
        "train_upos = [[token['upos'] for token in senc] for senc in train_sentences]\n",
        "\n",
        "test_tokens = [[token['lemma'] for token in senc] for senc in test_sentences]\n",
        "test_upos = [[token['upos'] for token in senc] for senc in test_sentences]"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wJU43_TqJ8U"
      },
      "source": [
        "### Explore dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "FINZUbYroJen",
        "outputId": "cc6f8590-ca41-4d00-b9f2-c0c301fce93c"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "train_df = pd.DataFrame({'tokens': train_tokens, 'upos': train_upos})\n",
        "print(train_df.info())\n",
        "train_df.head()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3997 entries, 0 to 3996\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   tokens  3997 non-null   object\n",
            " 1   upos    3997 non-null   object\n",
            "dtypes: object(2)\n",
            "memory usage: 62.6+ KB\n",
            "None\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tokens</th>\n",
              "      <th>upos</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[看似, 简单, ，, 只, 是, 二, 选, 一, 做, 决择, ，, 但, 其实, 他,...</td>\n",
              "      <td>[AUX, ADJ, PUNCT, ADV, VERB, NUM, VERB, NUM, V...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[其, 便当, 都是, 买来, 的, ，, 就算, 加热, 也是, 由, 妈妈, 负责, （...</td>\n",
              "      <td>[PRON, NOUN, VERB, VERB, PART, PUNCT, ADP, VER...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[这, 次, 游行, 最大, 的, 特色, ，, 在, 于, 越来越, 多, 年轻, 人, ...</td>\n",
              "      <td>[DET, NOUN, NOUN, ADJ, PART, NOUN, PUNCT, VERB...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[怀孕, 期, 为, 421, 至, 457, 日, 。]</td>\n",
              "      <td>[VERB, PART, AUX, NUM, CCONJ, NUM, NOUN, PUNCT]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[婷婷, 向, 昏迷, 中, 的, 婆婆, 诉说, ，, 为, 什么, 生活, 会, 与, ...</td>\n",
              "      <td>[PROPN, ADP, VERB, ADP, PART, NOUN, VERB, PUNC...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              tokens                                               upos\n",
              "0  [看似, 简单, ，, 只, 是, 二, 选, 一, 做, 决择, ，, 但, 其实, 他,...  [AUX, ADJ, PUNCT, ADV, VERB, NUM, VERB, NUM, V...\n",
              "1  [其, 便当, 都是, 买来, 的, ，, 就算, 加热, 也是, 由, 妈妈, 负责, （...  [PRON, NOUN, VERB, VERB, PART, PUNCT, ADP, VER...\n",
              "2  [这, 次, 游行, 最大, 的, 特色, ，, 在, 于, 越来越, 多, 年轻, 人, ...  [DET, NOUN, NOUN, ADJ, PART, NOUN, PUNCT, VERB...\n",
              "3                      [怀孕, 期, 为, 421, 至, 457, 日, 。]    [VERB, PART, AUX, NUM, CCONJ, NUM, NOUN, PUNCT]\n",
              "4  [婷婷, 向, 昏迷, 中, 的, 婆婆, 诉说, ，, 为, 什么, 生活, 会, 与, ...  [PROPN, ADP, VERB, ADP, PART, NOUN, VERB, PUNC..."
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "NIMUitvCZ1CU",
        "outputId": "631d6b8c-8970-49ad-8246-808441750a65"
      },
      "source": [
        "test_df = pd.DataFrame({'tokens': test_tokens, 'upos': test_upos})\n",
        "print(test_df.info())\n",
        "test_df.head()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 500 entries, 0 to 499\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   tokens  500 non-null    object\n",
            " 1   upos    500 non-null    object\n",
            "dtypes: object(2)\n",
            "memory usage: 7.9+ KB\n",
            "None\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tokens</th>\n",
              "      <th>upos</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[然而, ，, 这样, 的, 处理, 也, 衍生, 了, 一些, 问题, 。]</td>\n",
              "      <td>[ADV, PUNCT, PRON, PART, NOUN, ADV, VERB, AUX,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[自从, 2004, 年, 提出, 了, 兴建, 人文, 大楼, 的, 构想, ，, 企业,...</td>\n",
              "      <td>[ADP, NUM, NOUN, VERB, AUX, VERB, PROPN, NOUN,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[杜鹃花, 为, 温带, 植物, ，, 台北, 虽然, 在, 亚, 热带, ，, 但, 冬季...</td>\n",
              "      <td>[NOUN, AUX, NOUN, NOUN, PUNCT, PROPN, ADP, VER...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[台大, 医学, 人文, 博物, 馆, 是, 一, 栋, 两, 层, 楼, 的, 建筑, ，...</td>\n",
              "      <td>[PROPN, NOUN, NOUN, NOUN, PART, AUX, NUM, NOUN...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[楼顶, 有, 天文, 台, ，, 现, 为, 天文, 社, 使用, 。]</td>\n",
              "      <td>[NOUN, VERB, NOUN, PART, PUNCT, NOUN, VERB, NO...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              tokens                                               upos\n",
              "0            [然而, ，, 这样, 的, 处理, 也, 衍生, 了, 一些, 问题, 。]  [ADV, PUNCT, PRON, PART, NOUN, ADV, VERB, AUX,...\n",
              "1  [自从, 2004, 年, 提出, 了, 兴建, 人文, 大楼, 的, 构想, ，, 企业,...  [ADP, NUM, NOUN, VERB, AUX, VERB, PROPN, NOUN,...\n",
              "2  [杜鹃花, 为, 温带, 植物, ，, 台北, 虽然, 在, 亚, 热带, ，, 但, 冬季...  [NOUN, AUX, NOUN, NOUN, PUNCT, PROPN, ADP, VER...\n",
              "3  [台大, 医学, 人文, 博物, 馆, 是, 一, 栋, 两, 层, 楼, 的, 建筑, ，...  [PROPN, NOUN, NOUN, NOUN, PART, AUX, NUM, NOUN...\n",
              "4              [楼顶, 有, 天文, 台, ，, 现, 为, 天文, 社, 使用, 。]  [NOUN, VERB, NOUN, PART, PUNCT, NOUN, VERB, NO..."
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yE9bxxLldqTR",
        "outputId": "3c77f351-8af2-44e4-9c0c-36ce5f26326e"
      },
      "source": [
        "lengths = [len(tokens) for tokens in train_tokens] + [len(tokens) for tokens in test_tokens]\n",
        "print(min(lengths), max(lengths), sum(lengths) / len(lengths))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4 111 24.600400266844563\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HffcLoOxXZrt",
        "outputId": "0d1f74d7-2003-4cf8-a8a7-71161d4bc966"
      },
      "source": [
        "vocab = list(set([x for sublist in train_tokens for x in sublist] +\n",
        "            [x for sublist in test_tokens for x in sublist]))\n",
        "upos = list(set([x for sublist in train_upos for x in sublist] +\n",
        "           [x for sublist in test_upos for x in sublist]))\n",
        "print(len(vocab), len(upos))\n",
        "print(upos)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "18836 15\n",
            "['NUM', 'X', 'PUNCT', 'PRON', 'VERB', 'NOUN', 'PART', 'DET', 'SYM', 'ADJ', 'AUX', 'ADP', 'CCONJ', 'ADV', 'PROPN']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zghj8-91qx7N"
      },
      "source": [
        "### Super simple pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lzDoH2aDcnoM",
        "outputId": "3b102846-ce62-4839-b50a-d70e8b23b917"
      },
      "source": [
        "word_to_index = {}\n",
        "for i, w in enumerate(vocab):\n",
        "  word_to_index[w] = i\n",
        "\n",
        "upos_to_index = {}\n",
        "for i, tag in enumerate(upos):\n",
        "  upos_to_index[tag] = i\n",
        "\n",
        "upos_to_index"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'ADJ': 9,\n",
              " 'ADP': 11,\n",
              " 'ADV': 13,\n",
              " 'AUX': 10,\n",
              " 'CCONJ': 12,\n",
              " 'DET': 7,\n",
              " 'NOUN': 5,\n",
              " 'NUM': 0,\n",
              " 'PART': 6,\n",
              " 'PRON': 3,\n",
              " 'PROPN': 14,\n",
              " 'PUNCT': 2,\n",
              " 'SYM': 8,\n",
              " 'VERB': 4,\n",
              " 'X': 1}"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9akCf4iLg7am"
      },
      "source": [
        "train_tokens_idx = [[word_to_index[w] for w in senc] for senc in train_tokens]\n",
        "train_upos_idx = [[upos_to_index[tag] for tag in tags] for tags in train_upos]\n",
        "test_tokens_idx = [[word_to_index[w] for w in senc] for senc in test_tokens]\n",
        "test_upos_idx = [[upos_to_index[tag] for tag in tags] for tags in test_upos]"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXSTs-nBrCht"
      },
      "source": [
        "## Super simple LSTM tagger"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIQ7GDAgrJn4"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XgWxpqTgc8uu"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class LSTMTagger(nn.Module):\n",
        "  def __init__(self, embedding_dim, hidden_dim, vocab_size, target_size):\n",
        "    super(LSTMTagger, self).__init__()\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "    self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
        "    self.hidden2tag = nn.Linear(hidden_dim, target_size)\n",
        "      \n",
        "  def forward(self, sentence):\n",
        "    embeds = self.word_embeddings(sentence)\n",
        "    lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
        "    tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
        "    tag_scores = F.log_softmax(tag_space, dim=1)\n",
        "    return tag_scores"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D524w_RarMhS"
      },
      "source": [
        "### Train and Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvNrrVhjfzpv"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def train(model, x, y, optimizer, loss_function):\n",
        "  total_loss = 0.0\n",
        "  for senc, tags in zip(x, y):\n",
        "    model.zero_grad()\n",
        "    tags_score = model(torch.from_numpy(np.array(senc)))\n",
        "    loss = loss_function(tags_score, torch.from_numpy(np.array(tags)))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    total_loss += loss.item()\n",
        "  return total_loss / len(y)\n",
        "\n",
        "def test(model, x, y, loss_function):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for senc, tags in zip(x, y):\n",
        "          tags_score = model(torch.from_numpy(np.array(senc)))\n",
        "          loss = loss_function(tags_score, torch.from_numpy(np.array(tags)))\n",
        "          total_loss += loss.item()\n",
        "    return total_loss / len(y)"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTvjb3jHfx85",
        "outputId": "e32270b9-6e9d-4daf-f7b8-84b4e39cfabc"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "EMBEDDING_DIM = 64\n",
        "HIDDEN_DIM = 64\n",
        "\n",
        "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(vocab), len(upos))\n",
        "loss_function = nn.NLLLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "for epoch in range(10):\n",
        "  train_loss = train(model, train_tokens_idx, train_upos_idx, optimizer, loss_function)\n",
        "  test_loss = test(model, test_tokens_idx, test_upos_idx, loss_function)\n",
        "  print(f'Epoch {epoch} loss: \\t\\t train: {train_loss} \\t\\t test: {test_loss}')\n"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 loss: \t\t train: 1.3586804690079477 \t\t test: 1.0206133653521539\n",
            "Epoch 1 loss: \t\t train: 0.8965861461711789 \t\t test: 0.8310876260995865\n",
            "Epoch 2 loss: \t\t train: 0.7405388850783046 \t\t test: 0.7569588625580073\n",
            "Epoch 3 loss: \t\t train: 0.6495578677796828 \t\t test: 0.7214270924776792\n",
            "Epoch 4 loss: \t\t train: 0.5840074673657577 \t\t test: 0.702707530245185\n",
            "Epoch 5 loss: \t\t train: 0.5322766246743387 \t\t test: 0.6935067253559828\n",
            "Epoch 6 loss: \t\t train: 0.48887401277980885 \t\t test: 0.6896170590221882\n",
            "Epoch 7 loss: \t\t train: 0.4509073142985183 \t\t test: 0.6898233030550182\n",
            "Epoch 8 loss: \t\t train: 0.4167668779277165 \t\t test: 0.6938297143559903\n",
            "Epoch 9 loss: \t\t train: 0.3856248107515341 \t\t test: 0.701374791732058\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5TIaj5I9rlDy",
        "outputId": "bc82d377-81ec-49c6-f910-a6a382196d08"
      },
      "source": [
        "from random import choice\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "idx = choice(range(len(test_upos_idx)))\n",
        "tags = test_upos_idx[idx]\n",
        "senc = test_tokens_idx[idx]\n",
        "\n",
        "tag_score = model(torch.from_numpy(np.array(senc)))\n",
        "_, idxes = torch.max(tag_score, 1)\n",
        "predicted_tags = [upos[i.item()] for i in idxes]\n",
        "\n",
        "print(''.join(test_tokens[idx]))\n",
        "print(predicted_tags)\n",
        "print(test_upos[idx])\n",
        "print(f1_score(tags, [i.item() for i in idxes], average='weighted'))"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "人建造了一个纪念公园，并且为受害者放置了纪念石凳。\n",
            "['NOUN', 'VERB', 'AUX', 'NUM', 'NOUN', 'NOUN', 'NOUN', 'PUNCT', 'ADV', 'ADP', 'PROPN', 'PART', 'VERB', 'AUX', 'NOUN', 'NOUN', 'PUNCT']\n",
            "['NOUN', 'VERB', 'AUX', 'NUM', 'NOUN', 'NOUN', 'NOUN', 'PUNCT', 'ADV', 'ADP', 'VERB', 'PART', 'VERB', 'AUX', 'NOUN', 'NOUN', 'PUNCT']\n",
            "0.9647058823529411\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5_GdaJHpQlU"
      },
      "source": [
        "## References\n",
        "\n",
        "* [Kaggle note book: LSTM (character + word) POS-tag model PyTorch ](https://www.kaggle.com/krishanudb/lstm-character-word-pos-tag-model-pytorch)"
      ]
    }
  ]
}