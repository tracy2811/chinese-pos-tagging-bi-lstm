# Chinese Part-of-Speech tagging with bi-LSTM attention network

**Full name**: Trang Nguyen

**Group**: BS-18-DS-01

**Repo**: [https://github.com/tracy2811/chinese-pos-tagging-bi-lstm](https://github.com/tracy2811/chinese-pos-tagging-bi-lstm)

**Dataset**: [https://github.com/UniversalDependencies/UD_Chinese-GSDSimp/tree/master](https://github.com/UniversalDependencies/UD_Chinese-GSDSimp/tree/master)

**Notebook**: [notebook.ipynb](https://github.com/tracy2811/chinese-pos-tagging-bi-lstm/notebook.ipynb)

------------------------

D1-2 is focusing on:

1. Download, read and explore dataset

1. Build simple model

-------------------------

## Exploring dataset

| | Train | Test | Total
--- | --- | --- | ---
#sentences | 3997 | 500 | 4497

| | Min | Max | Average
--- | --- | --- | ---
#tokens in sentence | 4 | 111 | 24.6

| | Token | Tags 
--- | --- | --- 
#unique | 18836 | 15
Values | | 'NUM', 'X', 'PUNCT', 'PRON', 'VERB', 'NOUN', 'PART', 'DET', 'SYM', 'ADJ', 'AUX', 'ADP', 'CCONJ', 'ADV', 'PROPN'



| | Column 2 | Column 3
--- | --- | ---
**Things** | _Don't_ | [Need](http://makeuseof.com)
To | *__Look__* | `Pretty`

## Super simple pre-processing

1. Build vocabulary, and tag list

2. Map token, and tag from each sentence to their index in vocabulary and tag list

## Super simple LSTM tagger


### Structure

![LSTM tagger structure](https://github.com/tracy2811/chinese-pos-tagging-bi-lstm/report/lstm-tagger.png)


### Losses

```
Epoch 0 loss: 		 train: 1.3586804690079477 		 test: 1.0206133653521539
Epoch 1 loss: 		 train: 0.8965861461711789 		 test: 0.8310876260995865
Epoch 2 loss: 		 train: 0.7405388850783046 		 test: 0.7569588625580073
Epoch 3 loss: 		 train: 0.6495578677796828 		 test: 0.7214270924776792
Epoch 4 loss: 		 train: 0.5840074673657577 		 test: 0.702707530245185
Epoch 5 loss: 		 train: 0.5322766246743387 		 test: 0.6935067253559828
Epoch 6 loss: 		 train: 0.48887401277980885 		 test: 0.6896170590221882
Epoch 7 loss: 		 train: 0.4509073142985183 		 test: 0.6898233030550182
Epoch 8 loss: 		 train: 0.4167668779277165 		 test: 0.6938297143559903
Epoch 9 loss: 		 train: 0.3856248107515341 		 test: 0.701374791732058
```

### Random result


Sentence |  人 / 建造 / 了 / 一 / 个 / 纪念 / 公园 / ， / 并且 / 为 / 受害 / 者 / 放置 / 了 / 纪念 / 石凳 / 。
--- | ---
True tags | NOUN / VERB / AUX / NUM / NOUN / NOUN / NOUN / PUNCT / ADV / ADP / VERB / PART / VERB / AUX / NOUN / NOUN / PUNCT
Predicted tags | NOUN / VERB / AUX / NUM / NOUN / NOUN / NOUN / PUNCT / ADV / ADP / PROPN / PART / VERB / AUX / NOUN / NOUN / PUNCT
F1 for sentence| 0.9647058823529411

## References

* [https://www.kaggle.com/krishanudb/lstm-character-word-pos-tag-model-pytorch](https://www.kaggle.com/krishanudb/lstm-character-word-pos-tag-model-pytorch)
